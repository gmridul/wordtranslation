{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Nearest_Neighbor' from 'D:\\\\UCSD\\\\F17\\\\CSE293\\\\wordtranslation\\\\model\\\\Nearest_Neighbor.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import Nearest_Neighbor as nn\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import importlib\n",
    "importlib.reload(nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_file = \"D:/UCSD/F17/CSE293/data_prep_scripts/train_data.npy\"\n",
    "train_data = np.load(train_data_file)\n",
    "test_data_file = \"D:/UCSD/F17/CSE293/data_prep_scripts/test_data.npy\"\n",
    "test_data = np.load(test_data_file)\n",
    "\n",
    "\n",
    "#train_data_normalized_file = \"D:/UCSD/F17/CSE293/data_prep_scripts/train_data_normalized.npy\"\n",
    "#train_data_normalized = np.load(train_data_normalized_file)\n",
    "\n",
    "#test_data_normalized_file = \"D:/UCSD/F17/CSE293/data_prep_scripts/test_data_normalized.npy\"\n",
    "#test_data_normalized = np.load(test_data_normalized_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#row-wise normalizing make sure that the data is arranged rowwise\n",
    "####### ran once to preprocess.\n",
    "\n",
    "def normalize(X):\n",
    "    X = (X / np.linalg.norm(X, axis=1)[:,None])\n",
    "    return X\n",
    "\n",
    "for lang_data in test_data: #each language\n",
    "    for item_e in enumerate(lang_data): #for each example\n",
    "        i, item = item_e\n",
    "        lang_data[i] = normalize(item) #[[source], [target]]\n",
    "        \n",
    "np.save('D:/UCSD/F17/CSE293/data_prep_scripts/test_data_normalized.npy', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_ne = nn.Nearest_Neighbor(embeddings_dir=\"D:/UCSD/F17/CSE293/\", domain_dir='D:/UCSD/F17/CSE293/wordtranslation/model/data/', lang_list=['es', 'pt'], k_neighbors=10, flag='test')\n",
    "             \n",
    "nearest_ne_train = dict()\n",
    "'''lang_set = set()\n",
    "for lang_pairs in train_lang_pairs:\n",
    "    langs = llang_set = set()\n",
    "for lang_pairs in train_lang_pairs:\n",
    "    langs = lang_pairs.split('-')\n",
    "    lang_set.update(langs)\n",
    "ang_pairs.split('-')\n",
    "    lang_set.update(langs)\n",
    "'''\n",
    "\n",
    "nearest_ne_train = nn.Nearest_Neighbor(embeddings_dir=\"D:/UCSD/F17/CSE293\", domain_dir='D:/UCSD/F17/CSE293/wordtranslation/model/data/', lang_list=['pt','es','en'], k_neighbors=80, flag='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(point, cur_point):\n",
    "    return np.linalg.norm(point-cur_point)\n",
    "\n",
    "def farthest_points(row, cur_point):\n",
    "    dist_list = np.apply_along_axis(dist, 1, row, cur_point)\n",
    "    indices = np.argsort(dist_list)\n",
    "    return row[indices[80//2:]]\n",
    "\n",
    "def match(nn_list, y):\n",
    "    if y in nn_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#increase gamma makes model more strict on distance from negative expample i.e. farther from negative, closer to actual\n",
    "# k is number of negative examples to be passed to calculate loss\n",
    "class model(object):\n",
    "    def __init__(self, ckpt_path, lr, dim_lang=8, dim_model=300, k=40, gamma=0.3, model_name=\"MLPM\"):\n",
    "        #self.epochs = epochs\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.model_name = model_name\n",
    "        self.lr = lr\n",
    "        self.dim_lang = dim_lang\n",
    "        self.dim_model = dim_model\n",
    "        self.all_lang_rep_enc = dict()\n",
    "        self.all_lang_rep_dec = dict()\n",
    "        self.k = k\n",
    "        for lang in [\"en\", \"pt\", \"es\"]:\n",
    "            self.all_lang_rep_enc[lang] = np.random.normal(loc=0.0, scale=1/np.sqrt(float(dim_lang)), size=(dim_lang))\n",
    "            self.all_lang_rep_dec[lang] = np.random.normal(loc=0.0, scale=1/np.sqrt(float(dim_lang)), size=(dim_lang))\n",
    "        \n",
    "        def __graph__():\n",
    "            tf.reset_default_graph()\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "\n",
    "                # source and target vector representation\n",
    "                self.source_words = tf.placeholder(tf.float32, shape=[None, dim_model])\n",
    "                self.target_words = tf.placeholder(tf.float32, shape=[None, dim_model])\n",
    "\n",
    "                self.max_margin_neighbors = tf.placeholder(tf.float32, shape=[None, self.k, dim_model])\n",
    "                # parameter matrices\n",
    "                self.encoder = tf.Variable(tf.truncated_normal([dim_model, dim_model],\\\n",
    "                                           stddev=1/tf.sqrt(float(dim_model))), name='encoder')\n",
    "\n",
    "                self.decoder = tf.Variable(tf.truncated_normal([dim_model, dim_model],\\\n",
    "                                           stddev=1/tf.sqrt(float(dim_model))), name='decoder')\n",
    "\n",
    "                #todo:: second dimension can be different that dim_model\n",
    "                self.lang_encoder = tf.Variable(tf.truncated_normal([dim_lang,dim_model],\\\n",
    "                                               stddev=1/tf.sqrt(float(dim_model))), name='lang_encoder')\n",
    "                self.lang_decoder = tf.Variable(tf.truncated_normal([dim_lang,dim_model],\\\n",
    "                                               stddev=1/tf.sqrt(float(dim_model))), name='lang_decoder')\n",
    "\n",
    "                # language representations\n",
    "                self.lang_rep_enc_placeholder = tf.placeholder(tf.float32, shape=[1,self.dim_lang])\n",
    "                self.lang_rep_enc = tf.Variable(tf.truncated_normal([1, self.dim_lang],\\\n",
    "                                           stddev=1/tf.sqrt(1.0*dim_lang)))\n",
    "                self.assign_lang_rep_enc_op = self.lang_rep_enc.assign(self.lang_rep_enc_placeholder)\n",
    "                \n",
    "                self.lang_rep_dec_placeholder = tf.placeholder(tf.float32, shape=[1,self.dim_lang])\n",
    "                self.lang_rep_dec = tf.Variable(tf.truncated_normal([1, self.dim_lang],\\\n",
    "                                           stddev=1/tf.sqrt(1.0*dim_lang)))\n",
    "                self.assign_lang_rep_dec_op = self.lang_rep_dec.assign(self.lang_rep_dec_placeholder)\n",
    "\n",
    "                # model equation\n",
    "                self.target_pred = self.get_model(self.encoder, self.decoder, self.source_words,\\\n",
    "                                                  self.lang_rep_enc, self.lang_rep_dec, self.lang_encoder, self.lang_decoder)\n",
    "\n",
    "                # cosine distance between target_words and predicted target words\n",
    "                # A: None*1\n",
    "                A = tf.losses.cosine_distance(tf.nn.l2_normalize(self.target_words,dim=1),\\\n",
    "                                              tf.nn.l2_normalize(self.target_pred,dim=1),\\\n",
    "                                              dim=1, reduction=\"none\")\n",
    "                # repeat A k times to use it later for max-margin\n",
    "                # B: None*k\n",
    "                B = tf.tile(A, [1, self.k])\n",
    "\n",
    "                # repeat target_pred k times on row\n",
    "                # target_pred_k: None*k*dim_model\n",
    "                \n",
    "                #todo:: make target pred normalized before repeat\n",
    "                #target_pred_normalized = tf.nn.l2_normalize(self.target_pred,dim=1)\n",
    "                target_pred_k = tf.tile(self.target_pred,[1, self.k])\n",
    "                target_pred_k = tf.reshape(target_pred_k, [tf.shape(self.target_pred)[0], self.k, dim_model])\n",
    "\n",
    "                target_pred_k_normalize = tf.nn.l2_normalize(target_pred_k, dim=2)\n",
    "\n",
    "                max_margin_neighbors_normalize = tf.nn.l2_normalize(self.max_margin_neighbors, dim=2)\n",
    "\n",
    "\n",
    "                # cosine distance between target_pred_normalize and k negative targets\n",
    "                # C: None*k*1 and reshape to None*k\n",
    "                C = tf.losses.cosine_distance(target_pred_k_normalize, max_margin_neighbors_normalize, dim=2)\n",
    "                C = tf.reshape(C, tf.shape(C)[:2])\n",
    "\n",
    "                #max-margin loss\n",
    "                #self.loss = tf.reduce_sum(tf.maximum(0.,B-C+gamma))\n",
    "                #squared loss\n",
    "                self.loss = tf.reduce_sum(tf.square(self.target_words-self.target_pred))\n",
    "                #self.train_step = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss)\n",
    "                self.train_step = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "                self.init = tf.global_variables_initializer()\n",
    "                self.saver = tf.train.Saver()\n",
    "        \n",
    "        print('start building graph')\n",
    "        __graph__()\n",
    "        print('graph built')\n",
    "        \n",
    "    # encoder, decoder: 300x300   source_words: Nonex300    lang_rep: 1xdim_lang    lang_coder: dim_langx300\n",
    "    # Output: Nonex300\n",
    "    # we calculate the final projection in two parts  \n",
    "    def get_model(self, encoder, decoder, source_words, lang_rep_enc, lang_rep_dec, lang_encoder, lang_decoder):\n",
    "        #source to shared using encoder\n",
    "        shared_source_words = tf.matmul(source_words, encoder)\n",
    "        shared_lang_rep_enc = tf.matmul(lang_rep_enc, lang_encoder)\n",
    "        num_examples = tf.shape(shared_source_words)[0]\n",
    "        #(shared_source_words.get_shape())[0]\n",
    "        tile = tf.tile(shared_lang_rep_enc, tf.convert_to_tensor([1, num_examples]))\n",
    "        shared_lang_aux = tf.reshape(tile, [num_examples, self.dim_model])\n",
    "        shared_embedding_vector = shared_source_words + shared_lang_aux\n",
    "        \n",
    "        #shared to target using decoder\n",
    "        decoded_word_base = tf.matmul(shared_embedding_vector, decoder)\n",
    "        shared_lang_rep_dec = tf.matmul(lang_rep_dec, lang_decoder)\n",
    "        tile = tf.tile(shared_lang_rep_dec, tf.convert_to_tensor([1, num_examples]))\n",
    "        shared_lang_aux = tf.reshape(tile, [num_examples, self.dim_model])\n",
    "        \n",
    "        ret = decoded_word_base + shared_lang_aux\n",
    "        return ret \n",
    "    \n",
    "    def get_feed(self, X, Y, src_lang, dest_lang, Z= None):        \n",
    "        feed_dict = {self.source_words: X, self.target_words: Y}\n",
    "        if (Z is not None):\n",
    "            feed_dict[self.max_margin_neighbors] = Z\n",
    "        return feed_dict\n",
    "\n",
    "#     def get_prediction(self, nearest_ne, lang, pred, target, k_neighbors):\n",
    "#         def dist(point, cur_point):\n",
    "#             return np.linalg.norm(point-cur_point)\n",
    "\n",
    "#         def f(row, cur_point):\n",
    "#             dist_list = np.apply_along_axis(dist, 1, row, cur_point)\n",
    "#             indices = np.argsort(dist_list)\n",
    "#             return row[indices[len(indices)//2:]]\n",
    "        \n",
    "#         nearest_neighbors_pred = nearest_ne.knn(pred, lang, k_neighbors)\n",
    "#         farthest_neighbors_target =[]\n",
    "#         for cur_point,row in zip(target, nearest_neighbors_pred):\n",
    "#             farthest_neighbors_target.append(f(row, cur_point))\n",
    "        \n",
    "#         return farthest_neighbors_target\n",
    "        \n",
    "    #train -> train[en_pt]\n",
    "    def train(self, train, train_lang_pairs, batch_size, validation, test, test_lang_pairs,nearest_ne, nearest_ne_train ,\\\n",
    "              num_epochs=10, sess=None, log_file=\"./log_file.txt\"):\n",
    "        self.test_loss_array = []\n",
    "        if sess == None:\n",
    "            sess = tf.Session(graph = self.graph)\n",
    "            sess.run(self.init)\n",
    "                            \n",
    "        with sess.as_default():\n",
    "            assert sess is tf.get_default_session()\n",
    "            \n",
    "            #max data for a language pair/batch size\n",
    "        max_data_size = int(max([len(x) for x in train])/batch_size)\n",
    "        batch_counter_outer = 0\n",
    "        try:\n",
    "            with open(log_file, 'w') as log :\n",
    "                log.write(\"Batch \\t lang_pair \\t loss\\n\")\n",
    "\n",
    "                for epoch in range(num_epochs):\n",
    "                    batch_index = np.zeros(shape=(len(train)), dtype=int)\n",
    "                    for batch_number in range(max_data_size):\n",
    "                        #make batches\n",
    "                        batch_counter_outer += 1\n",
    "\n",
    "                        for lang_pair_data_enum in enumerate(train):\n",
    "                            log_string = \"\"\n",
    "                            i, lang_pair_data = lang_pair_data_enum\n",
    "                            lang_pair_data = np.array(lang_pair_data)\n",
    "\n",
    "                            #rounding on the language pairs with less words in dictionary\n",
    "                            if batch_index[i] >= len(lang_pair_data):\n",
    "                                np.random.shuffle(train[i])\n",
    "                                batch_index[i] = 0\n",
    "\n",
    "                            cur_batch = lang_pair_data[batch_index[i]:(batch_index[i]+batch_size),:,:]\n",
    "                            batch_index[i] += batch_size\n",
    "\n",
    "                            X = cur_batch[:,0,:]\n",
    "                            Y = cur_batch[:,1,:]\n",
    "\n",
    "                            lang = train_lang_pairs[i].split('-')\n",
    "\n",
    "                            A = self.all_lang_rep_enc[lang[0]]\n",
    "                            B = self.all_lang_rep_dec[lang[1]]\n",
    "                            #init = np.append(A,B)\n",
    "\n",
    "                            # making shape to (1, 1* dim_lang) from (, 1*dim_lang )\n",
    "                            A = A.reshape(1, np.shape(A)[0])\n",
    "                            B = B.reshape(1, np.shape(B)[0])\n",
    "\n",
    "                            #assign the lang representation for a run\n",
    "                            sess_lang_rep_enc, sess_lang_rep_dec = sess.run([self.assign_lang_rep_enc_op, self.assign_lang_rep_enc_op] \\\n",
    "                                                          , feed_dict={self.lang_rep_enc_placeholder : A, self.lang_rep_dec_placeholder : B})\n",
    "\n",
    "                            # get the negative examples wrt the best heuristic (near to prediction and far from target)\n",
    "                            target_pred = sess.run([self.target_pred], self.get_feed(X, Y, lang[0], lang[1]))\n",
    "                            Z = self.get_prediction(nearest_ne_train, lang[1], target_pred, Y, 80)\n",
    "\n",
    "                            # Find k random negative samples for each data word in Y\n",
    "                            #Z = lang_pair_data[:,1,:][random.sample(range(len(lang_pair_data)),k)]\n",
    "\n",
    "                            #batch index more that lang pair data length not handled :P\n",
    "                            _, train_loss, lang_rep_enc, lang_rep_dec, encoder, decoder= \\\n",
    "                                        sess.run([self.train_step, self.loss, self.lang_rep_enc, self.lang_rep_dec, self.encoder, self.decoder],\\\n",
    "                                                 self.get_feed(X, Y, lang[0], lang[1], Z))\n",
    "\n",
    "                            log_string += str(batch_counter_outer) + \" \\t \" + train_lang_pairs[i] + \" \\t \" + str(train_loss/batch_size) + \"\\n\"\n",
    "                            log.write(log_string)\n",
    "\n",
    "                            self.all_lang_rep_enc[lang[0]], self.all_lang_rep_dec[lang[1]] = lang_rep_enc[0], lang_rep_dec[0]\n",
    "\n",
    "                            #test_code after every iteration\n",
    "                            if (batch_counter_outer % 20 == 0):\n",
    "                                self.test(test, test_lang_pairs, nearest_ne, sess)\n",
    "                            #\n",
    "                            #---------------------------------------------------------\n",
    "                            #print(\"encoder : \", encoder)\n",
    "                            #print(\"decoder : \", decoder)\n",
    "                            #print(\"lang_encoder : \", lang_encoder)\n",
    "                            #print(\"lang rep : \", lang_rep)\n",
    "                            #print (\"error : \", train_loss)\n",
    "                            #if (lang[0] == 'pt'):\n",
    "                            #    print(\"previous lang rep: \", A)\n",
    "                            #    print(\"new lang rep :\", lang_rep[0][:self.dim_lang])\n",
    "\n",
    "                            #print (lang[0], lang_rep[0][:5])\n",
    "                            #print (lang[1], lang_rep[0][5:])\n",
    "                            print(\"Batch:\" + str(batch_counter_outer))\n",
    "                            #-----------------------------------------------------------\n",
    "                    #save epoch\n",
    "                    if epoch and epoch%10==0:\n",
    "                        self.saver.save(sess, self.ckpt_path+self.model_name+\".ckpt\", global_step=epoch)\n",
    "\n",
    "            #print losses #todo: format for train data with lang\n",
    "            self.saver.save(sess, self.ckpt_path+self.model_name+\".ckpt\", global_step=num_epochs+1)\n",
    "                #np.savetxt()\n",
    "        except KeyboardInterrupt:\n",
    "            print ('Interupted by keyboard')\n",
    "            self.saver.save(sess, self.ckpt_path+self.model_name+\".ckpt\", global_step=epoch)\n",
    "\n",
    "    def get_prediction(self, nearest_ne, lang, pred, target, k_neighbors):        \n",
    "        #print(np.shape(pred))\n",
    "        nearest_neighbors_pred = nearest_ne.knn(pred[0], lang, k_neighbors)\n",
    "        \n",
    "        farthest_neighbors_target = Parallel(n_jobs=1)(delayed(farthest_points)(row, cur_point) for cur_point,row in zip(target, nearest_neighbors_pred))\n",
    "                \n",
    "        return np.array(farthest_neighbors_target)\n",
    "    \n",
    "    \n",
    "    def restore_last_session(self):\n",
    "        saver = tf.train.Saver()\n",
    "        # create a session\n",
    "        sess = tf.Session()\n",
    "        # get checkpoint state\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckpt_path)\n",
    "        # restore session\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        # return to user\n",
    "        return sess\n",
    "\n",
    "    def test(self, test, test_lang_pairs, nearest_neighbor, sess, log_file=\"./test_log_file.txt\"):\n",
    "        for lang_pair_data_enum in enumerate(test):\n",
    "            i, lang_pair_data = lang_pair_data_enum\n",
    "            lang_pair_data = np.array(lang_pair_data)\n",
    "            \n",
    "            lang = test_lang_pairs[i].split('-')\n",
    "            A = self.all_lang_rep_enc[lang[0]]\n",
    "            B = self.all_lang_rep_dec[lang[1]]\n",
    "            \n",
    "                        \n",
    "            # making shape to (1, 1* dim_lang) from (, 1*dim_lang )\n",
    "            A = A.reshape(1, np.shape(A)[0])\n",
    "            B = B.reshape(1, np.shape(B)[0])\n",
    "\n",
    "            #assign the lang representation for a run\n",
    "            #sess_lang_rep = sess.run(self.assign_lang_rep_op, feed_dict={self.lang_rep_placeholder : init})\n",
    "            sess_lang_rep_enc, sess_lang_rep_dec = sess.run([self.assign_lang_rep_enc_op, self.assign_lang_rep_enc_op] \\\n",
    "                                                          , feed_dict={self.lang_rep_enc_placeholder : A, self.lang_rep_dec_placeholder : B})\n",
    "            \n",
    "            X = lang_pair_data[:,0,:]\n",
    "            Y = lang_pair_data[:,1,:]\n",
    "            \n",
    "            # Find k random negative samples for each data word in Y\n",
    "            #todo: change this to nearest neighbor hueristic\n",
    "            target_pred = sess.run([self.target_pred], self.get_feed(X, Y, lang[0], lang[1]))\n",
    "            Z = self.get_prediction(nearest_ne_train, lang[1], target_pred, Y, 80)\n",
    "            \n",
    "            #Z = lang_pair_data[:,1,:][random.sample(range(len(lang_pair_data)), self.k)]\n",
    "            #Z = np.tile(Z, (len(Y),1,1))\n",
    "            #print (np.shape(Z))\n",
    "            #print (Z)\n",
    "            #batch index more that lang pair data length not handled :P\n",
    "            test_loss, target_pred = sess.run([self.loss, self.target_pred], self.get_feed(X, Y, lang[0], lang[1], Z))\n",
    "            self.test_loss_array.append(test_loss)\n",
    "            near_points = nearest_neighbor.knn(target_pred, lang[1], 10)\n",
    "            self.calculate_precision(near_points, Y)\n",
    "            print('--------------------')\n",
    "            print(lang)\n",
    "            print(\"Test loss: \", test_loss)\n",
    "            \n",
    "            '''print('P@1: ', nn(1))\n",
    "            print('P@5: ', nn(5))\n",
    "            print('P@10: ', nn(10))'''\n",
    "            break\n",
    "     \n",
    "    def calculate_precision(self, predictions_nn, actual):\n",
    "        pos = 0\n",
    "        total = len(actual)\n",
    "        #print(\"actual:\", actual[0])\n",
    "        #print(\"pred:\", predictions_nn[0][0])            \n",
    "        pos = np.sum(Parallel(n_jobs=1)(delayed(match)(nn_list, y) for nn_list, y in zip(predictions_nn, actual)))\n",
    "        '''\n",
    "        for (i, y) in enumerate(actual):\n",
    "            if y in predictions_nn[i]:\n",
    "                pos+=1\n",
    "        '''\n",
    "        print(str(pos) + \"out of \" + str(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start building graph\n",
      "graph built\n"
     ]
    }
   ],
   "source": [
    "mlpm = model(ckpt_path=\"./ckpt/\",lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:1\n",
      "Batch:1\n",
      "Batch:1\n",
      "Batch:2\n",
      "Batch:2\n",
      "Batch:2\n",
      "Batch:3\n",
      "Batch:3\n",
      "Batch:3\n",
      "Batch:4\n",
      "Batch:4\n",
      "Batch:4\n",
      "Batch:5\n",
      "Batch:5\n",
      "Batch:5\n",
      "Batch:6\n",
      "Batch:6\n",
      "Batch:6\n",
      "Batch:7\n",
      "Batch:7\n",
      "Batch:7\n",
      "Batch:8\n",
      "Batch:8\n",
      "Batch:8\n",
      "Batch:9\n",
      "Batch:9\n",
      "Batch:9\n",
      "Batch:10\n",
      "Batch:10\n",
      "Batch:10\n",
      "Batch:11\n",
      "Batch:11\n",
      "Batch:11\n",
      "Batch:12\n",
      "Batch:12\n",
      "Batch:12\n",
      "Batch:13\n",
      "Batch:13\n",
      "Batch:13\n",
      "Batch:14\n",
      "Batch:14\n",
      "Batch:14\n",
      "Batch:15\n",
      "Batch:15\n",
      "Batch:15\n",
      "Batch:16\n",
      "Batch:16\n",
      "Batch:16\n",
      "Batch:17\n",
      "Batch:17\n",
      "Batch:17\n",
      "Batch:18\n",
      "Batch:18\n",
      "Batch:18\n",
      "Batch:19\n",
      "Batch:19\n",
      "Batch:19\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1558.88\n",
      "Batch:20\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1557.8\n",
      "Batch:20\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1556.81\n",
      "Batch:20\n",
      "Batch:21\n",
      "Batch:21\n",
      "Batch:21\n",
      "Batch:22\n",
      "Batch:22\n",
      "Batch:22\n",
      "Batch:23\n",
      "Batch:23\n",
      "Batch:23\n",
      "Batch:24\n",
      "Batch:24\n",
      "Batch:24\n",
      "Batch:25\n",
      "Batch:25\n",
      "Batch:25\n",
      "Batch:26\n",
      "Batch:26\n",
      "Batch:26\n",
      "Batch:27\n",
      "Batch:27\n",
      "Batch:27\n",
      "Batch:28\n",
      "Batch:28\n",
      "Batch:28\n",
      "Batch:29\n",
      "Batch:29\n",
      "Batch:29\n",
      "Batch:30\n",
      "Batch:30\n",
      "Batch:30\n",
      "Batch:31\n",
      "Batch:31\n",
      "Batch:31\n",
      "Batch:32\n",
      "Batch:32\n",
      "Batch:32\n",
      "Batch:33\n",
      "Batch:33\n",
      "Batch:33\n",
      "Batch:34\n",
      "Batch:34\n",
      "Batch:34\n",
      "Batch:35\n",
      "Batch:35\n",
      "Batch:35\n",
      "Batch:36\n",
      "Batch:36\n",
      "Batch:36\n",
      "Batch:37\n",
      "Batch:37\n",
      "Batch:37\n",
      "Batch:38\n",
      "Batch:38\n",
      "Batch:38\n",
      "Batch:39\n",
      "Batch:39\n",
      "Batch:39\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1475.84\n",
      "Batch:40\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1487.97\n",
      "Batch:40\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1499.49\n",
      "Batch:40\n",
      "Batch:41\n",
      "Batch:41\n",
      "Batch:41\n",
      "Batch:42\n",
      "Batch:42\n",
      "Batch:42\n",
      "Batch:43\n",
      "Batch:43\n",
      "Batch:43\n",
      "Batch:44\n",
      "Batch:44\n",
      "Batch:44\n",
      "Batch:45\n",
      "Batch:45\n",
      "Batch:45\n",
      "Batch:46\n",
      "Batch:46\n",
      "Batch:46\n",
      "Batch:47\n",
      "Batch:47\n",
      "Batch:47\n",
      "Batch:48\n",
      "Batch:48\n",
      "Batch:48\n",
      "Batch:49\n",
      "Batch:49\n",
      "Batch:49\n",
      "Batch:50\n",
      "Batch:50\n",
      "Batch:50\n",
      "Batch:51\n",
      "Batch:51\n",
      "Batch:51\n",
      "Batch:52\n",
      "Batch:52\n",
      "Batch:52\n",
      "Batch:53\n",
      "Batch:53\n",
      "Batch:53\n",
      "Batch:54\n",
      "Batch:54\n",
      "Batch:54\n",
      "Batch:55\n",
      "Batch:55\n",
      "Batch:55\n",
      "Batch:56\n",
      "Batch:56\n",
      "Batch:56\n",
      "Batch:57\n",
      "Batch:57\n",
      "Batch:57\n",
      "Batch:58\n",
      "Batch:58\n",
      "Batch:58\n",
      "Batch:59\n",
      "Batch:59\n",
      "Batch:59\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1459.68\n",
      "Batch:60\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1469.63\n",
      "Batch:60\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1479.36\n",
      "Batch:60\n",
      "Batch:61\n",
      "Batch:61\n",
      "Batch:61\n",
      "Batch:62\n",
      "Batch:62\n",
      "Batch:62\n",
      "Batch:63\n",
      "Batch:63\n",
      "Batch:63\n",
      "Batch:64\n",
      "Batch:64\n",
      "Batch:64\n",
      "Batch:65\n",
      "Batch:65\n",
      "Batch:65\n",
      "Batch:66\n",
      "Batch:66\n",
      "Batch:66\n",
      "Batch:67\n",
      "Batch:67\n",
      "Batch:67\n",
      "Batch:68\n",
      "Batch:68\n",
      "Batch:68\n",
      "Batch:69\n",
      "Batch:69\n",
      "Batch:69\n",
      "Batch:70\n",
      "Batch:70\n",
      "Batch:70\n",
      "Batch:71\n",
      "Batch:71\n",
      "Batch:71\n",
      "Batch:72\n",
      "Batch:72\n",
      "Batch:72\n",
      "Batch:73\n",
      "Batch:73\n",
      "Batch:73\n",
      "Batch:74\n",
      "Batch:74\n",
      "Batch:74\n",
      "Batch:75\n",
      "Batch:75\n",
      "Batch:75\n",
      "Batch:76\n",
      "Batch:76\n",
      "Batch:76\n",
      "Batch:77\n",
      "Batch:77\n",
      "Batch:77\n",
      "Batch:78\n",
      "Batch:78\n",
      "Batch:78\n",
      "Batch:79\n",
      "Batch:79\n",
      "Batch:79\n",
      "3out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1478.3\n",
      "Batch:80\n",
      "3out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1492.39\n",
      "Batch:80\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1505.24\n",
      "Batch:80\n",
      "Batch:81\n",
      "Batch:81\n",
      "Batch:81\n",
      "Batch:82\n",
      "Batch:82\n",
      "Batch:82\n",
      "Batch:83\n",
      "Batch:83\n",
      "Batch:83\n",
      "Batch:84\n",
      "Batch:84\n",
      "Batch:84\n",
      "Batch:85\n",
      "Batch:85\n",
      "Batch:85\n",
      "Batch:86\n",
      "Batch:86\n",
      "Batch:86\n",
      "Batch:87\n",
      "Batch:87\n",
      "Batch:87\n",
      "Batch:88\n",
      "Batch:88\n",
      "Batch:88\n",
      "Batch:89\n",
      "Batch:89\n",
      "Batch:89\n",
      "Batch:90\n",
      "Batch:90\n",
      "Batch:90\n",
      "Batch:91\n",
      "Batch:91\n",
      "Batch:91\n",
      "Batch:92\n",
      "Batch:92\n",
      "Batch:92\n",
      "Batch:93\n",
      "Batch:93\n",
      "Batch:93\n",
      "Batch:94\n",
      "Batch:94\n",
      "Batch:94\n",
      "Batch:95\n",
      "Batch:95\n",
      "Batch:95\n",
      "Batch:96\n",
      "Batch:96\n",
      "Batch:96\n",
      "Batch:97\n",
      "Batch:97\n",
      "Batch:97\n",
      "Batch:98\n",
      "Batch:98\n",
      "Batch:98\n",
      "Batch:99\n",
      "Batch:99\n",
      "Batch:99\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1497.67\n",
      "Batch:100\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1498.47\n",
      "Batch:100\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1499.02\n",
      "Batch:100\n",
      "Batch:101\n",
      "Batch:101\n",
      "Batch:101\n",
      "Batch:102\n",
      "Batch:102\n",
      "Batch:102\n",
      "Batch:103\n",
      "Batch:103\n",
      "Batch:103\n",
      "Batch:104\n",
      "Batch:104\n",
      "Batch:104\n",
      "Batch:105\n",
      "Batch:105\n",
      "Batch:105\n",
      "Batch:106\n",
      "Batch:106\n",
      "Batch:106\n",
      "Batch:107\n",
      "Batch:107\n",
      "Batch:107\n",
      "Batch:108\n",
      "Batch:108\n",
      "Batch:108\n",
      "Batch:109\n",
      "Batch:109\n",
      "Batch:109\n",
      "Batch:110\n",
      "Batch:110\n",
      "Batch:110\n",
      "Batch:111\n",
      "Batch:111\n",
      "Batch:111\n",
      "Batch:112\n",
      "Batch:112\n",
      "Batch:112\n",
      "Batch:113\n",
      "Batch:113\n",
      "Batch:113\n",
      "Batch:114\n",
      "Batch:114\n",
      "Batch:114\n",
      "Batch:115\n",
      "Batch:115\n",
      "Batch:115\n",
      "Batch:116\n",
      "Batch:116\n",
      "Batch:116\n",
      "Batch:117\n",
      "Batch:117\n",
      "Batch:117\n",
      "Batch:118\n",
      "Batch:118\n",
      "Batch:118\n",
      "Batch:119\n",
      "Batch:119\n",
      "Batch:119\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1534.8\n",
      "Batch:120\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1541.22\n",
      "Batch:120\n",
      "2out of 86\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1546.59\n",
      "Batch:120\n",
      "Batch:121\n",
      "Batch:121\n",
      "Batch:121\n",
      "Batch:122\n",
      "Batch:122\n",
      "Batch:122\n",
      "Batch:123\n",
      "Batch:123\n",
      "Batch:123\n",
      "Batch:124\n",
      "Batch:124\n",
      "Batch:124\n",
      "Batch:125\n",
      "Batch:125\n",
      "Batch:125\n",
      "Interupted by keyboard\n"
     ]
    }
   ],
   "source": [
    "#mlpm.train(train_data_normalized,train_lang_pairs= [\"pt-es\", \"pt-en\", \"en-es\"], batch_size=40, validation=[], test= test_data_normalized, test_lang_pairs=[\"pt-es\"])\n",
    "mlpm.train(train_data,train_lang_pairs= [\"pt-es\", \"pt-en\", \"en-es\"], batch_size=40, validation=[], test= test_data, test_lang_pairs=[\"pt-es\"], nearest_ne=nearest_ne, nearest_ne_train=nearest_ne_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD8CAYAAAB6paOMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x270bb419550>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(mlpm.test_loss_array)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
