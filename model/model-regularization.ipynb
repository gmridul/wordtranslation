{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import Nearest_Neighbor as nn\n",
    "import random\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import importlib\n",
    "importlib.reload(nn)\n",
    "\n",
    "cur_user = 'g'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_profiles = {}\n",
    "user_profiles['g'] = {}\n",
    "user_profiles['g']['parallel_jobs'] = 1\n",
    "user_profiles['g']['data_file_base'] = 'D:/UCSD/F17/CSE293/data_prep_scripts/'\n",
    "user_profiles['g']['embed_dir'] = 'D:/UCSD/F17/CSE293/'\n",
    "user_profiles['g']['domain_dir'] = 'D:/UCSD/F17/CSE293/wordtranslation/model/data/'\n",
    "user_profile = user_profiles[cur_user] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data_file = user_profile['data_file_base'] + \"train_data.npy\"\n",
    "train_data = np.load(train_data_file)\n",
    "test_data_file =  user_profile['data_file_base'] + \"test_data.npy\"\n",
    "test_data = np.load(test_data_file)\n",
    "\n",
    "\n",
    "#train_data_normalized_file = \"D:/UCSD/F17/CSE293/data_prep_scripts/train_data_normalized.npy\"\n",
    "#train_data_normalized = np.load(train_data_normalized_file)\n",
    "\n",
    "#test_data_normalized_file = \"D:/UCSD/F17/CSE293/data_prep_scripts/test_data_normalized.npy\"\n",
    "#test_data_normalized = np.load(test_data_normalized_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#row-wise normalizing make sure that the data is arranged rowwise\n",
    "####### ran once to preprocess.\n",
    "\n",
    "def normalize(X):\n",
    "    X = (X / np.linalg.norm(X, axis=1)[:,None])\n",
    "    return X\n",
    "\n",
    "for lang_data in test_data: #each language\n",
    "    for item_e in enumerate(lang_data): #for each example\n",
    "        i, item = item_e\n",
    "        lang_data[i] = normalize(item) #[[source], [target]]\n",
    "        \n",
    "np.save('D:/UCSD/F17/CSE293/data_prep_scripts/test_data_normalized.npy', test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nearest_ne = nn.Nearest_Neighbor(embeddings_dir= user_profile['embed_dir'], domain_dir= user_profile['domain_dir'], lang_list=['es', 'pt', 'en'], k_neighbors=20, flag='test')\n",
    "             \n",
    "nearest_ne_train = dict()\n",
    "'''lang_set = set()\n",
    "for lang_pairs in train_lang_pairs:\n",
    "    langs = llang_set = set()\n",
    "for lang_pairs in train_lang_pairs:\n",
    "    langs = lang_pairs.split('-')\n",
    "    lang_set.update(langs)\n",
    "ang_pairs.split('-')\n",
    "    lang_set.update(langs)\n",
    "'''\n",
    "\n",
    "nearest_ne_train = nn.Nearest_Neighbor(embeddings_dir= user_profile['embed_dir'], domain_dir= user_profile['domain_dir'], lang_list=['pt','es', 'en'], k_neighbors=80, flag='train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dist(point, cur_point):\n",
    "    return np.linalg.norm(point-cur_point)\n",
    "\n",
    "def farthest_points(row, cur_point):\n",
    "    dist_list = np.apply_along_axis(dist, 1, row, cur_point)\n",
    "    indices = np.argsort(dist_list)\n",
    "    return row[indices[80//2:]]\n",
    "\n",
    "def match(nn_list, y):\n",
    "    if y in nn_list:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#increase gamma makes model more strict on distance from negative expample i.e. farther from negative, closer to actual\n",
    "# k is number of negative examples to be passed to calculate loss\n",
    "class model(object):\n",
    "    def __init__(self, ckpt_path, lr, dim_lang=5, dim_model=300, k=40, gamma=0.7, model_name=\"MLPM\", lam=0.0001):\n",
    "        #self.epochs = epochs\n",
    "        self.ckpt_path = ckpt_path\n",
    "        self.model_name = model_name\n",
    "        self.lr = lr\n",
    "        self.dim_lang = dim_lang\n",
    "        self.dim_model = dim_model\n",
    "        self.all_lang_rep_enc = dict()\n",
    "        self.all_lang_rep_dec = dict()\n",
    "        self.k = k\n",
    "        for lang in [\"en\", \"pt\", \"es\"]:\n",
    "            self.all_lang_rep_enc[lang] = np.random.normal(loc=0.0, scale=1/np.sqrt(float(dim_lang)), size=(dim_lang))\n",
    "            self.all_lang_rep_dec[lang] = np.random.normal(loc=0.0, scale=1/np.sqrt(float(dim_lang)), size=(dim_lang))\n",
    "        \n",
    "        def __graph__():\n",
    "            tf.reset_default_graph()\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "\n",
    "                # source and target vector representation\n",
    "                self.source_words = tf.placeholder(tf.float32, shape=[None, dim_model+1])\n",
    "                self.target_words = tf.placeholder(tf.float32, shape=[None, dim_model])\n",
    "\n",
    "                self.max_margin_neighbors = tf.placeholder(tf.float32, shape=[None, self.k, dim_model])\n",
    "                # parameter matrices\n",
    "                self.encoder = tf.Variable(tf.truncated_normal([dim_model+1, dim_model],\\\n",
    "                                           stddev=1/tf.sqrt(float(dim_model))), name='encoder')\n",
    "\n",
    "                self.decoder = tf.Variable(tf.truncated_normal([dim_model+1, dim_model],\\\n",
    "                                           stddev=1/tf.sqrt(float(dim_model))), name='decoder')\n",
    "\n",
    "                #todo:: second dimension can be different that dim_model\n",
    "                self.lang_encoder = tf.Variable(tf.truncated_normal([dim_lang,dim_model],\\\n",
    "                                               stddev=1/tf.sqrt(float(dim_model))), name='lang_encoder')\n",
    "                self.lang_decoder = tf.Variable(tf.truncated_normal([dim_lang,dim_model],\\\n",
    "                                               stddev=1/tf.sqrt(float(dim_model))), name='lang_decoder')\n",
    "\n",
    "                # language representations\n",
    "                self.lang_rep_enc_placeholder = tf.placeholder(tf.float32, shape=[1,self.dim_lang])\n",
    "                self.lang_rep_enc = tf.Variable(tf.truncated_normal([1, self.dim_lang],\\\n",
    "                                           stddev=1/tf.sqrt(1.0*dim_lang)))\n",
    "                self.assign_lang_rep_enc_op = self.lang_rep_enc.assign(self.lang_rep_enc_placeholder)\n",
    "                \n",
    "                self.lang_rep_dec_placeholder = tf.placeholder(tf.float32, shape=[1,self.dim_lang])\n",
    "                self.lang_rep_dec = tf.Variable(tf.truncated_normal([1, self.dim_lang],\\\n",
    "                                           stddev=1/tf.sqrt(1.0*dim_lang)))\n",
    "                self.assign_lang_rep_dec_op = self.lang_rep_dec.assign(self.lang_rep_dec_placeholder)\n",
    "\n",
    "                # model equation\n",
    "                self.target_pred = self.get_model(self.encoder, self.decoder, self.source_words,\\\n",
    "                                                  self.lang_rep_enc, self.lang_rep_dec, self.lang_encoder, self.lang_decoder)\n",
    "\n",
    "                # cosine distance between target_words and predicted target words\n",
    "                # A: None*1\n",
    "                #A = tf.losses.cosine_distance(tf.nn.l2_normalize(self.target_words,dim=1),\\\n",
    "                #                              tf.nn.l2_normalize(self.target_pred,dim=1),\\\n",
    "                #                              dim=1, reduction=\"none\")\n",
    "                A = tf.norm(self.target_words-self.target_pred, axis=1)\n",
    "                A = tf.reshape(A, [tf.shape(A)[0] ,1])\n",
    "                # repeat A k times to use it later for max-margin\n",
    "                # B: None*k\n",
    "                B = tf.tile(A, [1, self.k])\n",
    "\n",
    "                # repeat target_pred k times on row\n",
    "                # target_pred_k: None*k*dim_model\n",
    "                \n",
    "                #todo:: make target pred normalized before repeat\n",
    "                #target_pred_normalized = tf.nn.l2_normalize(self.target_pred,dim=1)\n",
    "                target_pred_k = tf.tile(self.target_pred,[1, self.k])\n",
    "                target_pred_k = tf.reshape(target_pred_k, [tf.shape(self.target_pred)[0], self.k, dim_model])\n",
    "\n",
    "                #target_pred_k_normalize = tf.nn.l2_normalize(target_pred_k, dim=2)\n",
    "\n",
    "                #max_margin_neighbors_normalize = tf.nn.l2_normalize(self.max_margin_neighbors, dim=2)\n",
    "\n",
    "\n",
    "                # cosine distance between target_pred_normalize and k negative targets\n",
    "                # C: None*k*1 and reshape to None*k\n",
    "                #C = tf.losses.cosine_distance(target_pred_k_normalize, max_margin_neighbors_normalize, dim=2)\n",
    "                #C = tf.reshape(C, tf.shape(C)[:2])\n",
    "                \n",
    "                C = tf.norm(target_pred_k - self.max_margin_neighbors, axis=2)\n",
    "                C = tf.reshape(C, tf.shape(C)[:2])\n",
    "\n",
    "                #max-margin loss\n",
    "                regularizer = tf.nn.l2_loss(self.encoder)+tf.nn.l2_loss(self.decoder)+\\\n",
    "                              tf.nn.l2_loss(self.lang_encoder)+tf.nn.l2_loss(self.lang_decoder)\n",
    "                    \n",
    "                self.loss = tf.reduce_sum(tf.maximum(0.,B-C+gamma)+lam*regularizer)\n",
    "                #squared loss\n",
    "                #self.loss = tf.reduce_sum(tf.square(self.target_words-self.target_pred))\n",
    "\n",
    "                #self.train_step = tf.train.GradientDescentOptimizer(self.lr).minimize(self.loss)\n",
    "                self.train_step = tf.train.AdamOptimizer(learning_rate=self.lr).minimize(self.loss)\n",
    "                self.saver = tf.train.Saver()\n",
    "                self.init = tf.global_variables_initializer()\n",
    "        print('start building graph')\n",
    "        __graph__()\n",
    "        print('graph built')\n",
    "        \n",
    "    # encoder, decoder: 300x300   source_words: Nonex300    lang_rep: 1xdim_lang    lang_coder: dim_langx300\n",
    "    # Output: Nonex300\n",
    "    # we calculate the final projection in two parts  \n",
    "    def get_model(self, encoder, decoder, source_words, lang_rep_enc, lang_rep_dec, lang_encoder, lang_decoder):\n",
    "        #source to shared using encoder\n",
    "        shared_source_words = tf.matmul(source_words, encoder)\n",
    "        shared_lang_rep_enc = tf.matmul(lang_rep_enc, lang_encoder)\n",
    "        num_examples = tf.shape(shared_source_words)[0]\n",
    "        #(shared_source_words.get_shape())[0]\n",
    "        tile = tf.tile(shared_lang_rep_enc, tf.convert_to_tensor([1, num_examples]))\n",
    "        shared_lang_aux = tf.reshape(tile, [num_examples, self.dim_model])\n",
    "        shared_embedding_vector = (shared_source_words + shared_lang_aux)\n",
    "        \n",
    "        ones = tf.ones([tf.shape(shared_embedding_vector)[0],1], tf.float32)\n",
    "        shared_embedding_vector = tf.concat([ones, shared_embedding_vector], axis=1)\n",
    "        #shared to target using decoder\n",
    "        decoded_word_base = tf.matmul(shared_embedding_vector, decoder)\n",
    "        shared_lang_rep_dec = tf.matmul(lang_rep_dec, lang_decoder)\n",
    "        tile = tf.tile(shared_lang_rep_dec, tf.convert_to_tensor([1, num_examples]))\n",
    "        shared_lang_aux = tf.reshape(tile, [num_examples, self.dim_model])\n",
    "        \n",
    "        ret = (decoded_word_base + shared_lang_aux)\n",
    "        return ret \n",
    "    \n",
    "    def get_feed(self, X, Y, src_lang, dest_lang, Z= None): \n",
    "        X = np.insert(X, 0, 1, axis=1)\n",
    "        feed_dict = {self.source_words: X, self.target_words: Y}\n",
    "        if (Z is not None):\n",
    "            feed_dict[self.max_margin_neighbors] = Z\n",
    "        return feed_dict\n",
    "\n",
    "#     def get_prediction(self, nearest_ne, lang, pred, target, k_neighbors):\n",
    "#         def dist(point, cur_point):\n",
    "#             return np.linalg.norm(point-cur_point)\n",
    "\n",
    "#         def f(row, cur_point):\n",
    "#             dist_list = np.apply_along_axis(dist, 1, row, cur_point)\n",
    "#             indices = np.argsort(dist_list)\n",
    "#             return row[indices[len(indices)//2:]]\n",
    "        \n",
    "#         nearest_neighbors_pred = nearest_ne.knn(pred, lang, k_neighbors)\n",
    "#         farthest_neighbors_target =[]\n",
    "#         for cur_point,row in zip(target, nearest_neighbors_pred):\n",
    "#             farthest_neighbors_target.append(f(row, cur_point))\n",
    "        \n",
    "#         return farthest_neighbors_target\n",
    "        \n",
    "    #train -> train[en_pt]\n",
    "    def train(self, train, train_lang_pairs, batch_size, validation, test, test_lang_pairs,nearest_ne, nearest_ne_train ,\\\n",
    "              num_epochs=10, sess=None, log_file=\"./log_file.txt\"):\n",
    "        saver = self.saver\n",
    "        if sess == None:\n",
    "            sess = tf.Session(graph = self.graph)\n",
    "            sess.run(self.init)              \n",
    "        with sess.as_default():\n",
    "            assert sess is tf.get_default_session()\n",
    "            \n",
    "            #max data for a language pair/batch size\n",
    "            max_data_size = int(max([len(x) for x in train])/batch_size)\n",
    "            batch_counter_outer = 0\n",
    "            try:\n",
    "                with open(log_file, 'w') as log :\n",
    "                    log.write(\"Batch \\t lang_pair \\t loss\\n\")\n",
    "\n",
    "                    for epoch in range(num_epochs):\n",
    "                        batch_index = np.zeros(shape=(len(train)), dtype=int)\n",
    "                        for batch_number in range(max_data_size):\n",
    "                            #make batches\n",
    "                            batch_counter_outer += 1\n",
    "\n",
    "                            for lang_pair_data_enum in enumerate(train):\n",
    "                                log_string = \"\"\n",
    "                                i, lang_pair_data = lang_pair_data_enum\n",
    "                                lang_pair_data = np.array(lang_pair_data)\n",
    "\n",
    "                                #rounding on the language pairs with less words in dictionary\n",
    "                                if batch_index[i] >= len(lang_pair_data):\n",
    "                                    np.random.shuffle(train[i])\n",
    "                                    batch_index[i] = 0\n",
    "\n",
    "                                cur_batch = lang_pair_data[batch_index[i]:(batch_index[i]+batch_size),:,:]\n",
    "                                batch_index[i] += batch_size\n",
    "\n",
    "                                X = cur_batch[:,0,:]\n",
    "                                Y = cur_batch[:,1,:]\n",
    "\n",
    "                                lang = train_lang_pairs[i].split('-')\n",
    "\n",
    "                                A = self.all_lang_rep_enc[lang[0]]\n",
    "                                B = self.all_lang_rep_dec[lang[1]]\n",
    "                                #init = np.append(A,B)\n",
    "\n",
    "                                # making shape to (1, 1* dim_lang) from (, 1*dim_lang )\n",
    "                                A = A.reshape(1, np.shape(A)[0])\n",
    "                                B = B.reshape(1, np.shape(B)[0])\n",
    "\n",
    "                                #assign the lang representation for a run\n",
    "                                sess_lang_rep_enc, sess_lang_rep_dec = sess.run([self.assign_lang_rep_enc_op, self.assign_lang_rep_enc_op] \\\n",
    "                                                              , feed_dict={self.lang_rep_enc_placeholder : A, self.lang_rep_dec_placeholder : B})\n",
    "\n",
    "                                # get the negative examples wrt the best heuristic (near to prediction and far from target)\n",
    "                                target_pred = sess.run([self.target_pred], self.get_feed(X, Y, lang[0], lang[1]))\n",
    "                                Z = self.get_prediction(nearest_ne_train, lang[1], target_pred, Y, 80)\n",
    "\n",
    "                                # Find k random negative samples for each data word in Y\n",
    "                                #Z = lang_pair_data[:,1,:][random.sample(range(len(lang_pair_data)),k)]\n",
    "\n",
    "                                #batch index more that lang pair data length not handled :P\n",
    "                                _, train_loss, lang_rep_enc, lang_rep_dec, encoder, decoder= \\\n",
    "                                            sess.run([self.train_step, self.loss, self.lang_rep_enc, self.lang_rep_dec, self.encoder, self.decoder],\\\n",
    "                                                     self.get_feed(X, Y, lang[0], lang[1], Z))\n",
    "\n",
    "                                log_string += str(batch_counter_outer) + \" \\t \" + train_lang_pairs[i] + \" \\t \" + str(train_loss/batch_size) + \"\\n\"\n",
    "                                log.write(log_string)\n",
    "\n",
    "                                self.all_lang_rep_enc[lang[0]], self.all_lang_rep_dec[lang[1]] = lang_rep_enc[0], lang_rep_dec[0]\n",
    "\n",
    "                                #test_code after every iteration\n",
    "                                if (batch_counter_outer % 10 == 0):\n",
    "                                    print(\"Train Loss : \" + str(train_loss))\n",
    "                                    self.test(test, test_lang_pairs, nearest_ne, sess)\n",
    "                                #\n",
    "                                #---------------------------------------------------------\n",
    "                                #print(\"encoder : \", encoder)\n",
    "                                #print(\"decoder : \", decoder)\n",
    "                                #print(\"lang_encoder : \", lang_encoder)\n",
    "                                #print(\"lang rep : \", lang_rep)\n",
    "                                #print (\"error : \", train_loss)\n",
    "                                #if (lang[0] == 'pt'):\n",
    "                                #    print(\"previous lang rep: \", A)\n",
    "                                #    print(\"new lang rep :\", lang_rep[0][:self.dim_lang])\n",
    "\n",
    "                                #print (lang[0], lang_rep[0][:5])\n",
    "                                #print (lang[1], lang_rep[0][5:])\n",
    "                                print(\"Batch:\" + str(batch_counter_outer))\n",
    "                                #-----------------------------------------------------------\n",
    "                        #save epoch\n",
    "                        if epoch and epoch%10==0:\n",
    "                            saver.save(sess, self.ckpt_path+self.model_name+\".ckpt\", global_step=epoch)\n",
    "\n",
    "                #print losses #todo: format for train data with lang\n",
    "                #saver.save(sess, self.ckpt_path+self.model_name+\".ckpt\", global_step=num_epochs+1)\n",
    "                    #np.savetxt()\n",
    "            except KeyboardInterrupt:\n",
    "                print ('Interupted by keyboard')\n",
    "                saver.save(sess, self.ckpt_path+self.model_name+\".ckpt\", global_step=epoch)\n",
    "                self.session = sess\n",
    "                return sess\n",
    "\n",
    "    def get_prediction(self, nearest_ne, lang, pred, target, k_neighbors):        \n",
    "        #print(np.shape(pred))\n",
    "        nearest_neighbors_pred = nearest_ne.knn(pred[0], lang, k_neighbors)\n",
    "        \n",
    "        farthest_neighbors_target = Parallel(n_jobs= user_profile['parallel_jobs'])(delayed(farthest_points)(row, cur_point) for cur_point,row in zip(target, nearest_neighbors_pred))\n",
    "                \n",
    "        return np.array(farthest_neighbors_target)\n",
    "    \n",
    "    \n",
    "    def restore_last_session(self):\n",
    "        saver = self.saver #tf.train.Saver()\n",
    "        # create a session\n",
    "        sess = tf.Session(graph = self.graph) \n",
    "        # get checkpoint state\n",
    "        ckpt = tf.train.get_checkpoint_state(self.ckpt_path)\n",
    "        # restore session\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        # return to user\n",
    "        return sess\n",
    "\n",
    "    def test(self, test, test_lang_pairs, nearest_neighbor, sess, log_file=\"./test_log_file.txt\"):\n",
    "        for lang_pair_data_enum in enumerate(test):\n",
    "            i, lang_pair_data = lang_pair_data_enum\n",
    "            lang_pair_data = np.array(lang_pair_data)\n",
    "            \n",
    "            lang = test_lang_pairs[i].split('-')\n",
    "            A = self.all_lang_rep_enc[lang[0]]\n",
    "            B = self.all_lang_rep_dec[lang[1]]\n",
    "            \n",
    "                        \n",
    "            # making shape to (1, 1* dim_lang) from (, 1*dim_lang )\n",
    "            A = A.reshape(1, np.shape(A)[0])\n",
    "            B = B.reshape(1, np.shape(B)[0])\n",
    "\n",
    "            #assign the lang representation for a run\n",
    "            #sess_lang_rep = sess.run(self.assign_lang_rep_op, feed_dict={self.lang_rep_placeholder : init})\n",
    "            sess_lang_rep_enc, sess_lang_rep_dec = sess.run([self.assign_lang_rep_enc_op, self.assign_lang_rep_enc_op] \\\n",
    "                                                          , feed_dict={self.lang_rep_enc_placeholder : A, self.lang_rep_dec_placeholder : B})\n",
    "\n",
    "            \n",
    "            X = lang_pair_data[:,0,:]\n",
    "            Y = lang_pair_data[:,1,:]\n",
    "            \n",
    "            # Find k random negative samples for each data word in Y\n",
    "            #todo: change this to nearest neighbor hueristic\n",
    "            target_pred = sess.run([self.target_pred], self.get_feed(X, Y, lang[0], lang[1]))\n",
    "            Z = self.get_prediction(nearest_ne_train, lang[1], target_pred, Y, 80)\n",
    "            \n",
    "            #print (np.shape(Z))\n",
    "            #print (Z)\n",
    "            #batch index more that lang pair data length not handled :P\n",
    "            test_loss, target_pred = sess.run([self.loss, self.target_pred], self.get_feed(X, Y, lang[0], lang[1], Z))\n",
    "            near_points = nearest_neighbor.knn(target_pred, lang[1], 20)\n",
    "            self.calculate_precision(near_points, Y)\n",
    "            print('--------------------')\n",
    "            print(lang)\n",
    "            print(\"Test loss: \", test_loss)\n",
    "            \n",
    "            '''print('P@1: ', nn(1))\n",
    "            print('P@5: ', nn(5))\n",
    "            print('P@10: ', nn(10))'''\n",
    "            break\n",
    "     \n",
    "    def calculate_precision(self, predictions_nn, actual):\n",
    "        pos = 0\n",
    "        total = len(actual)\n",
    "        #print(\"actual:\", actual[0])\n",
    "        #print(\"pred:\", predictions_nn[0][0])            \n",
    "        pos = np.sum(Parallel(n_jobs=user_profile['parallel_jobs'])(delayed(match)(nn_list, y) for nn_list, y in zip(predictions_nn, actual)))\n",
    "        '''\n",
    "        for (i, y) in enumerate(actual):\n",
    "            if y in predictions_nn[i]:\n",
    "                pos+=1\n",
    "        '''\n",
    "        print(str(pos) + \"out of \" + str(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start building graph\n",
      "graph built\n"
     ]
    }
   ],
   "source": [
    "mlpm = model(ckpt_path=\"./ckpt/\",lr=0.001, lam=0.00000, dim_lang=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_ = train_data\n",
    "test_data_ = [train_data[0]]\n",
    "#sess = mlpm.restore_last_session()\n",
    "sess = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:1\n",
      "Batch:1\n",
      "Batch:1\n",
      "Batch:2\n",
      "Batch:2\n",
      "Batch:2\n",
      "Batch:3\n",
      "Batch:3\n",
      "Batch:3\n",
      "Batch:4\n",
      "Batch:4\n",
      "Batch:4\n",
      "Batch:5\n",
      "Batch:5\n",
      "Batch:5\n",
      "Batch:6\n",
      "Batch:6\n",
      "Batch:6\n",
      "Batch:7\n",
      "Batch:7\n",
      "Batch:7\n",
      "Batch:8\n",
      "Batch:8\n",
      "Batch:8\n",
      "Batch:9\n",
      "Batch:9\n",
      "Batch:9\n",
      "Train Loss : 696.32\n",
      "28out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  8188.58\n",
      "Batch:10\n",
      "Train Loss : 2866.19\n",
      "28out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  8111.65\n",
      "Batch:10\n",
      "Train Loss : 3743.19\n",
      "26out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  8044.74\n",
      "Batch:10\n",
      "Batch:11\n",
      "Batch:11\n",
      "Batch:11\n",
      "Batch:12\n",
      "Batch:12\n",
      "Batch:12\n",
      "Batch:13\n",
      "Batch:13\n",
      "Batch:13\n",
      "Batch:14\n",
      "Batch:14\n",
      "Batch:14\n",
      "Batch:15\n",
      "Batch:15\n",
      "Batch:15\n",
      "Batch:16\n",
      "Batch:16\n",
      "Batch:16\n",
      "Batch:17\n",
      "Batch:17\n",
      "Batch:17\n",
      "Batch:18\n",
      "Batch:18\n",
      "Batch:18\n",
      "Batch:19\n",
      "Batch:19\n",
      "Batch:19\n",
      "Train Loss : 382.98\n",
      "41out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  6336.26\n",
      "Batch:20\n",
      "Train Loss : 2271.52\n",
      "42out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  6310.48\n",
      "Batch:20\n",
      "Train Loss : 3092.83\n",
      "42out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  6286.28\n",
      "Batch:20\n",
      "Batch:21\n",
      "Batch:21\n",
      "Batch:21\n",
      "Batch:22\n",
      "Batch:22\n",
      "Batch:22\n",
      "Batch:23\n",
      "Batch:23\n",
      "Batch:23\n",
      "Batch:24\n",
      "Batch:24\n",
      "Batch:24\n",
      "Batch:25\n",
      "Batch:25\n",
      "Batch:25\n",
      "Batch:26\n",
      "Batch:26\n",
      "Batch:26\n",
      "Batch:27\n",
      "Batch:27\n",
      "Batch:27\n",
      "Batch:28\n",
      "Batch:28\n",
      "Batch:28\n",
      "Batch:29\n",
      "Batch:29\n",
      "Batch:29\n",
      "Train Loss : 380.187\n",
      "53out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  5249.62\n",
      "Batch:30\n",
      "Train Loss : 2116.48\n",
      "52out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  5224.02\n",
      "Batch:30\n",
      "Train Loss : 2957.14\n",
      "53out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  5203.35\n",
      "Batch:30\n",
      "Batch:31\n",
      "Batch:31\n",
      "Batch:31\n",
      "Batch:32\n",
      "Batch:32\n",
      "Batch:32\n",
      "Batch:33\n",
      "Batch:33\n",
      "Batch:33\n",
      "Batch:34\n",
      "Batch:34\n",
      "Batch:34\n",
      "Batch:35\n",
      "Batch:35\n",
      "Batch:35\n",
      "Batch:36\n",
      "Batch:36\n",
      "Batch:36\n",
      "Batch:37\n",
      "Batch:37\n",
      "Batch:37\n",
      "Batch:38\n",
      "Batch:38\n",
      "Batch:38\n",
      "Batch:39\n",
      "Batch:39\n",
      "Batch:39\n",
      "Train Loss : 310.273\n",
      "58out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  4032.93\n",
      "Batch:40\n",
      "Train Loss : 1684.74\n",
      "58out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  4008.8\n",
      "Batch:40\n",
      "Train Loss : 2903.5\n",
      "57out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  3987.7\n",
      "Batch:40\n",
      "Batch:41\n",
      "Batch:41\n",
      "Batch:41\n",
      "Batch:42\n",
      "Batch:42\n",
      "Batch:42\n",
      "Batch:43\n",
      "Batch:43\n",
      "Batch:43\n",
      "Batch:44\n",
      "Batch:44\n",
      "Batch:44\n",
      "Batch:45\n",
      "Batch:45\n",
      "Batch:45\n",
      "Batch:46\n",
      "Batch:46\n",
      "Batch:46\n",
      "Batch:47\n",
      "Batch:47\n",
      "Batch:47\n",
      "Batch:48\n",
      "Batch:48\n",
      "Batch:48\n",
      "Batch:49\n",
      "Batch:49\n",
      "Batch:49\n",
      "Train Loss : 336.337\n",
      "66out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  2946.84\n",
      "Batch:50\n",
      "Train Loss : 1299.01\n",
      "67out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  2933.7\n",
      "Batch:50\n",
      "Train Loss : 2816.54\n",
      "68out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  2926.82\n",
      "Batch:50\n",
      "Batch:51\n",
      "Batch:51\n",
      "Batch:51\n",
      "Batch:52\n",
      "Batch:52\n",
      "Batch:52\n",
      "Batch:53\n",
      "Batch:53\n",
      "Batch:53\n",
      "Batch:54\n",
      "Batch:54\n",
      "Batch:54\n",
      "Batch:55\n",
      "Batch:55\n",
      "Batch:55\n",
      "Batch:56\n",
      "Batch:56\n",
      "Batch:56\n",
      "Batch:57\n",
      "Batch:57\n",
      "Batch:57\n",
      "Batch:58\n",
      "Batch:58\n",
      "Batch:58\n",
      "Batch:59\n",
      "Batch:59\n",
      "Batch:59\n",
      "Train Loss : 217.442\n",
      "76out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  2098.56\n",
      "Batch:60\n",
      "Train Loss : 1149.43\n",
      "76out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  2085.53\n",
      "Batch:60\n",
      "Train Loss : 2742.15\n",
      "77out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  2077.09\n",
      "Batch:60\n",
      "Batch:61\n",
      "Batch:61\n",
      "Batch:61\n",
      "Batch:62\n",
      "Batch:62\n",
      "Batch:62\n",
      "Batch:63\n",
      "Batch:63\n",
      "Batch:63\n",
      "Batch:64\n",
      "Batch:64\n",
      "Batch:64\n",
      "Batch:65\n",
      "Batch:65\n",
      "Batch:65\n",
      "Batch:66\n",
      "Batch:66\n",
      "Batch:66\n",
      "Batch:67\n",
      "Batch:67\n",
      "Batch:67\n",
      "Batch:68\n",
      "Batch:68\n",
      "Batch:68\n",
      "Batch:69\n",
      "Batch:69\n",
      "Batch:69\n",
      "Train Loss : 130.982\n",
      "78out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1434.3\n",
      "Batch:70\n",
      "Train Loss : 765.571\n",
      "78out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1434.17\n",
      "Batch:70\n",
      "Train Loss : 2578.28\n",
      "78out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  1436.54\n",
      "Batch:70\n",
      "Batch:71\n",
      "Batch:71\n",
      "Batch:71\n",
      "Batch:72\n",
      "Batch:72\n",
      "Batch:72\n",
      "Batch:73\n",
      "Batch:73\n",
      "Batch:73\n",
      "Batch:74\n",
      "Batch:74\n",
      "Batch:74\n",
      "Batch:75\n",
      "Batch:75\n",
      "Batch:75\n",
      "Batch:76\n",
      "Batch:76\n",
      "Batch:76\n",
      "Batch:77\n",
      "Batch:77\n",
      "Batch:77\n",
      "Batch:78\n",
      "Batch:78\n",
      "Batch:78\n",
      "Batch:79\n",
      "Batch:79\n",
      "Batch:79\n",
      "Train Loss : 108.793\n",
      "85out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  975.199\n",
      "Batch:80\n",
      "Train Loss : 602.528\n",
      "86out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  974.774\n",
      "Batch:80\n",
      "Train Loss : 2628.19\n",
      "86out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  979.663\n",
      "Batch:80\n",
      "Batch:81\n",
      "Batch:81\n",
      "Batch:81\n",
      "Batch:82\n",
      "Batch:82\n",
      "Batch:82\n",
      "Batch:83\n",
      "Batch:83\n",
      "Batch:83\n",
      "Batch:84\n",
      "Batch:84\n",
      "Batch:84\n",
      "Batch:85\n",
      "Batch:85\n",
      "Batch:85\n",
      "Batch:86\n",
      "Batch:86\n",
      "Batch:86\n",
      "Batch:87\n",
      "Batch:87\n",
      "Batch:87\n",
      "Batch:88\n",
      "Batch:88\n",
      "Batch:88\n",
      "Batch:89\n",
      "Batch:89\n",
      "Batch:89\n",
      "Train Loss : 97.2596\n",
      "89out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  690.148\n",
      "Batch:90\n",
      "Train Loss : 353.648\n",
      "88out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  696.474\n",
      "Batch:90\n",
      "Train Loss : 2339.94\n",
      "88out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  706.964\n",
      "Batch:90\n",
      "Batch:91\n",
      "Batch:91\n",
      "Batch:91\n",
      "Batch:92\n",
      "Batch:92\n",
      "Batch:92\n",
      "Batch:93\n",
      "Batch:93\n",
      "Batch:93\n",
      "Batch:94\n",
      "Batch:94\n",
      "Batch:94\n",
      "Batch:95\n",
      "Batch:95\n",
      "Batch:95\n",
      "Batch:96\n",
      "Batch:96\n",
      "Batch:96\n",
      "Batch:97\n",
      "Batch:97\n",
      "Batch:97\n",
      "Batch:98\n",
      "Batch:98\n",
      "Batch:98\n",
      "Batch:99\n",
      "Batch:99\n",
      "Batch:99\n",
      "Train Loss : 51.7625\n",
      "87out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  467.614\n",
      "Batch:100\n",
      "Train Loss : 359.257\n",
      "87out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  469.019\n",
      "Batch:100\n",
      "Train Loss : 2554.48\n",
      "87out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  472.787\n",
      "Batch:100\n",
      "Batch:101\n",
      "Batch:101\n",
      "Batch:101\n",
      "Batch:102\n",
      "Batch:102\n",
      "Batch:102\n",
      "Batch:103\n",
      "Batch:103\n",
      "Batch:103\n",
      "Batch:104\n",
      "Batch:104\n",
      "Batch:104\n",
      "Batch:105\n",
      "Batch:105\n",
      "Batch:105\n",
      "Batch:106\n",
      "Batch:106\n",
      "Batch:106\n",
      "Batch:107\n",
      "Batch:107\n",
      "Batch:107\n",
      "Batch:108\n",
      "Batch:108\n",
      "Batch:108\n",
      "Batch:109\n",
      "Batch:109\n",
      "Batch:109\n",
      "Train Loss : 18.7516\n",
      "91out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  350.245\n",
      "Batch:110\n",
      "Train Loss : 174.924\n",
      "91out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  355.191\n",
      "Batch:110\n",
      "Train Loss : 2302.19\n",
      "91out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  362.1\n",
      "Batch:110\n",
      "Batch:111\n",
      "Batch:111\n",
      "Batch:111\n",
      "Batch:112\n",
      "Batch:112\n",
      "Batch:112\n",
      "Batch:113\n",
      "Batch:113\n",
      "Batch:113\n",
      "Batch:114\n",
      "Batch:114\n",
      "Batch:114\n",
      "Batch:115\n",
      "Batch:115\n",
      "Batch:115\n",
      "Batch:116\n",
      "Batch:116\n",
      "Batch:116\n",
      "Batch:117\n",
      "Batch:117\n",
      "Batch:117\n",
      "Batch:118\n",
      "Batch:118\n",
      "Batch:118\n",
      "Batch:119\n",
      "Batch:119\n",
      "Batch:119\n",
      "Train Loss : 21.7329\n",
      "92out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  283.375\n",
      "Batch:120\n",
      "Train Loss : 89.359\n",
      "92out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  287.007\n",
      "Batch:120\n",
      "Train Loss : 2408.87\n",
      "93out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  292.587\n",
      "Batch:120\n",
      "Batch:121\n",
      "Batch:121\n",
      "Batch:121\n",
      "Batch:122\n",
      "Batch:122\n",
      "Batch:122\n",
      "Batch:123\n",
      "Batch:123\n",
      "Batch:123\n",
      "Batch:124\n",
      "Batch:124\n",
      "Batch:124\n",
      "Batch:125\n",
      "Batch:125\n",
      "Batch:125\n",
      "Batch:126\n",
      "Batch:126\n",
      "Batch:126\n",
      "Batch:127\n",
      "Batch:127\n",
      "Batch:127\n",
      "Batch:128\n",
      "Batch:128\n",
      "Batch:128\n",
      "Batch:129\n",
      "Batch:129\n",
      "Batch:129\n",
      "Train Loss : 22.9252\n",
      "90out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  221.27\n",
      "Batch:130\n",
      "Train Loss : 108.68\n",
      "90out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  221.444\n",
      "Batch:130\n",
      "Train Loss : 2279.29\n",
      "90out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  223.254\n",
      "Batch:130\n",
      "Batch:131\n",
      "Batch:131\n",
      "Batch:131\n",
      "Batch:132\n",
      "Batch:132\n",
      "Batch:132\n",
      "Batch:133\n",
      "Batch:133\n",
      "Batch:133\n",
      "Batch:134\n",
      "Batch:134\n",
      "Batch:134\n",
      "Batch:135\n",
      "Batch:135\n",
      "Batch:135\n",
      "Batch:136\n",
      "Batch:136\n",
      "Batch:136\n",
      "Batch:137\n",
      "Batch:137\n",
      "Batch:137\n",
      "Batch:138\n",
      "Batch:138\n",
      "Batch:138\n",
      "Batch:139\n",
      "Batch:139\n",
      "Batch:139\n",
      "Train Loss : 11.4451\n",
      "89out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  186.232\n",
      "Batch:140\n",
      "Train Loss : 49.3338\n",
      "89out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  189.469\n",
      "Batch:140\n",
      "Train Loss : 2038.81\n",
      "89out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  193.415\n",
      "Batch:140\n",
      "Batch:141\n",
      "Batch:141\n",
      "Batch:141\n",
      "Batch:142\n",
      "Batch:142\n",
      "Batch:142\n",
      "Batch:143\n",
      "Batch:143\n",
      "Batch:143\n",
      "Batch:144\n",
      "Batch:144\n",
      "Batch:144\n",
      "Batch:145\n",
      "Batch:145\n",
      "Batch:145\n",
      "Batch:146\n",
      "Batch:146\n",
      "Batch:146\n",
      "Batch:147\n",
      "Batch:147\n",
      "Batch:147\n",
      "Batch:148\n",
      "Batch:148\n",
      "Batch:148\n",
      "Batch:149\n",
      "Batch:149\n",
      "Batch:149\n",
      "Train Loss : 29.7976\n",
      "87out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  173.575\n",
      "Batch:150\n",
      "Train Loss : 49.5567\n",
      "87out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  176.973\n",
      "Batch:150\n",
      "Train Loss : 1889.56\n",
      "87out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  181.945\n",
      "Batch:150\n",
      "Batch:151\n",
      "Batch:151\n",
      "Batch:151\n",
      "Batch:152\n",
      "Batch:152\n",
      "Batch:152\n",
      "Batch:153\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch:153\n",
      "Batch:153\n",
      "Batch:154\n",
      "Batch:154\n",
      "Batch:154\n",
      "Batch:155\n",
      "Batch:155\n",
      "Batch:155\n",
      "Batch:156\n",
      "Batch:156\n",
      "Batch:156\n",
      "Batch:157\n",
      "Batch:157\n",
      "Batch:157\n",
      "Batch:158\n",
      "Batch:158\n",
      "Batch:158\n",
      "Batch:159\n",
      "Batch:159\n",
      "Batch:159\n",
      "Train Loss : 18.3103\n",
      "88out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  182.61\n",
      "Batch:160\n",
      "Train Loss : 32.795\n",
      "88out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  184.246\n",
      "Batch:160\n",
      "Train Loss : 2000.1\n",
      "88out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  186.038\n",
      "Batch:160\n",
      "Batch:161\n",
      "Batch:161\n",
      "Batch:161\n",
      "Batch:162\n",
      "Batch:162\n",
      "Batch:162\n",
      "Batch:163\n",
      "Batch:163\n",
      "Batch:163\n",
      "Batch:164\n",
      "Batch:164\n",
      "Batch:164\n",
      "Batch:165\n",
      "Batch:165\n",
      "Batch:165\n",
      "Batch:166\n",
      "Batch:166\n",
      "Batch:166\n",
      "Batch:167\n",
      "Batch:167\n",
      "Batch:167\n",
      "Batch:168\n",
      "Batch:168\n",
      "Batch:168\n",
      "Batch:169\n",
      "Batch:169\n",
      "Batch:169\n",
      "Train Loss : 11.8333\n",
      "89out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  147.615\n",
      "Batch:170\n",
      "Train Loss : 26.1594\n",
      "89out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  149.237\n",
      "Batch:170\n",
      "Train Loss : 2133.02\n",
      "89out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  151.295\n",
      "Batch:170\n",
      "Batch:171\n",
      "Batch:171\n",
      "Batch:171\n",
      "Batch:172\n",
      "Batch:172\n",
      "Batch:172\n",
      "Batch:173\n",
      "Batch:173\n",
      "Batch:173\n",
      "Batch:174\n",
      "Batch:174\n",
      "Batch:174\n",
      "Batch:175\n",
      "Batch:175\n",
      "Batch:175\n",
      "Batch:176\n",
      "Batch:176\n",
      "Batch:176\n",
      "Batch:177\n",
      "Batch:177\n",
      "Batch:177\n",
      "Batch:178\n",
      "Batch:178\n",
      "Batch:178\n",
      "Batch:179\n",
      "Batch:179\n",
      "Batch:179\n",
      "Train Loss : 15.4264\n",
      "86out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  134.894\n",
      "Batch:180\n",
      "Train Loss : 49.6337\n",
      "86out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  135.088\n",
      "Batch:180\n",
      "Train Loss : 1880.74\n",
      "86out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  136.293\n",
      "Batch:180\n",
      "Batch:181\n",
      "Batch:181\n",
      "Batch:181\n",
      "Batch:182\n",
      "Batch:182\n",
      "Batch:182\n",
      "Batch:183\n",
      "Batch:183\n",
      "Batch:183\n",
      "Batch:184\n",
      "Batch:184\n",
      "Batch:184\n",
      "Batch:185\n",
      "Batch:185\n",
      "Batch:185\n",
      "Batch:186\n",
      "Batch:186\n",
      "Batch:186\n",
      "Batch:187\n",
      "Batch:187\n",
      "Batch:187\n",
      "Batch:188\n",
      "Batch:188\n",
      "Batch:188\n",
      "Batch:189\n",
      "Batch:189\n",
      "Batch:189\n",
      "Train Loss : 9.08742\n",
      "88out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  134.142\n",
      "Batch:190\n",
      "Train Loss : 37.7675\n",
      "88out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  135.207\n",
      "Batch:190\n",
      "Train Loss : 1933.75\n",
      "88out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  137.076\n",
      "Batch:190\n",
      "Batch:191\n",
      "Batch:191\n",
      "Batch:191\n",
      "Batch:192\n",
      "Batch:192\n",
      "Batch:192\n",
      "Batch:193\n",
      "Batch:193\n",
      "Batch:193\n",
      "Batch:194\n",
      "Batch:194\n",
      "Batch:194\n",
      "Batch:195\n",
      "Batch:195\n",
      "Batch:195\n",
      "Batch:196\n",
      "Batch:196\n",
      "Batch:196\n",
      "Batch:197\n",
      "Batch:197\n",
      "Batch:197\n",
      "Batch:198\n",
      "Batch:198\n",
      "Batch:198\n",
      "Batch:199\n",
      "Batch:199\n",
      "Batch:199\n",
      "Train Loss : 19.519\n",
      "84out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  128.685\n",
      "Batch:200\n",
      "Train Loss : 26.7944\n",
      "84out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  128.445\n",
      "Batch:200\n",
      "Train Loss : 1935.06\n",
      "84out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  129.764\n",
      "Batch:200\n",
      "Batch:201\n",
      "Batch:201\n",
      "Batch:201\n",
      "Batch:202\n",
      "Batch:202\n",
      "Batch:202\n",
      "Batch:203\n",
      "Batch:203\n",
      "Batch:203\n",
      "Batch:204\n",
      "Batch:204\n",
      "Batch:204\n",
      "Batch:205\n",
      "Batch:205\n",
      "Batch:205\n",
      "Batch:206\n",
      "Batch:206\n",
      "Batch:206\n",
      "Batch:207\n",
      "Batch:207\n",
      "Batch:207\n",
      "Batch:208\n",
      "Batch:208\n",
      "Batch:208\n",
      "Batch:209\n",
      "Batch:209\n",
      "Batch:209\n",
      "Train Loss : 12.1939\n",
      "87out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  123.496\n",
      "Batch:210\n",
      "Train Loss : 34.9577\n",
      "87out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  125.085\n",
      "Batch:210\n",
      "Train Loss : 2071.16\n",
      "87out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  127.042\n",
      "Batch:210\n",
      "Batch:211\n",
      "Batch:211\n",
      "Batch:211\n",
      "Batch:212\n",
      "Batch:212\n",
      "Batch:212\n",
      "Batch:213\n",
      "Batch:213\n",
      "Batch:213\n",
      "Batch:214\n",
      "Batch:214\n",
      "Batch:214\n",
      "Batch:215\n",
      "Batch:215\n",
      "Batch:215\n",
      "Batch:216\n",
      "Batch:216\n",
      "Batch:216\n",
      "Batch:217\n",
      "Batch:217\n",
      "Batch:217\n",
      "Batch:218\n",
      "Batch:218\n",
      "Batch:218\n",
      "Batch:219\n",
      "Batch:219\n",
      "Batch:219\n",
      "Train Loss : 14.0317\n",
      "90out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  138.92\n",
      "Batch:220\n",
      "Train Loss : 21.9173\n",
      "90out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  145.051\n",
      "Batch:220\n",
      "Train Loss : 1788.36\n",
      "89out of 303\n",
      "--------------------\n",
      "['pt', 'es']\n",
      "Test loss:  152.339\n",
      "Batch:220\n",
      "Batch:221\n",
      "Interupted by keyboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.client.session.Session at 0x28525e10a58>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mlpm.train(train_data_normalized,train_lang_pairs= [\"pt-es\", \"pt-en\", \"en-es\"], batch_size=40, validation=[], test= test_data_normalized, test_lang_pairs=[\"pt-es\"])\n",
    "mlpm.train(train_data_,train_lang_pairs= [\"pt-es\", \"pt-en\", \"en-es\"], num_epochs=100, batch_size=70, validation=[], test= test_data_, test_lang_pairs=[\"pt-es\"], nearest_ne=nearest_ne, nearest_ne_train=nearest_ne_train, sess = sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./ckpt/MLPM.ckpt-34\n"
     ]
    }
   ],
   "source": [
    "sess =  mlpm.restore_last_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[303, 415, 151632]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[len(train_data[i]) for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_error = []\n",
    "with open(\"result_final\") as f:\n",
    "    for l in f:\n",
    "        if \"Test loss\" in l:\n",
    "            #print (l.split())\n",
    "            test_error.append((float)(l.split()[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8188.58,\n",
       " 8111.65,\n",
       " 8044.74,\n",
       " 6336.26,\n",
       " 6310.48,\n",
       " 6286.28,\n",
       " 5249.62,\n",
       " 5224.02,\n",
       " 5203.35,\n",
       " 4032.93,\n",
       " 4008.8,\n",
       " 3987.7,\n",
       " 2946.84,\n",
       " 2933.7,\n",
       " 2926.82,\n",
       " 2098.56,\n",
       " 2085.53,\n",
       " 2077.09,\n",
       " 1434.3,\n",
       " 1434.17,\n",
       " 1436.54,\n",
       " 975.199,\n",
       " 974.774,\n",
       " 979.663,\n",
       " 690.148,\n",
       " 696.474,\n",
       " 706.964,\n",
       " 467.614,\n",
       " 469.019,\n",
       " 472.787,\n",
       " 350.245,\n",
       " 355.191,\n",
       " 362.1,\n",
       " 283.375,\n",
       " 287.007,\n",
       " 292.587,\n",
       " 221.27,\n",
       " 221.444,\n",
       " 223.254,\n",
       " 186.232,\n",
       " 189.469,\n",
       " 193.415,\n",
       " 173.575,\n",
       " 176.973,\n",
       " 181.945,\n",
       " 182.61,\n",
       " 184.246,\n",
       " 186.038,\n",
       " 147.615,\n",
       " 149.237,\n",
       " 151.295,\n",
       " 134.894,\n",
       " 135.088,\n",
       " 136.293,\n",
       " 134.142,\n",
       " 135.207,\n",
       " 137.076,\n",
       " 128.685,\n",
       " 128.445,\n",
       " 129.764,\n",
       " 123.496,\n",
       " 125.085,\n",
       " 127.042,\n",
       " 138.92,\n",
       " 145.051,\n",
       " 152.339]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJQ4RERkQJQ4RERmQ/x8pDYnmU470EQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x285603dbba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_error)\n",
    "plt.xlabel('iteration')\n",
    "plt.ylabel('loss')\n",
    "plt.title('testing for [pt-es] with training data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
